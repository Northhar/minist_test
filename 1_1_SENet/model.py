#!/usr/bin/env python3
"""
SENet (Squeeze-and-Excitation Networks) æ¨¡å‹å®ç°
"""

import numpy as np
import torch
from torch import nn
from torch.nn import init


class SEAttention(nn.Module):
    """
    Squeeze-and-Excitation Networks æ³¨æ„åŠ›æœºåˆ¶
    é€‚é…ç‰ˆæœ¬ï¼šå…ˆé€šè¿‡å·ç§¯å±‚å°†è¾“å…¥æ‰©å±•ä¸ºå¤šé€šé“ï¼Œç„¶ååº”ç”¨SEæ³¨æ„åŠ›æœºåˆ¶
    é€‚ç”¨äºMNISTç­‰å•é€šé“è¾“å…¥æ•°æ®
    """

    def __init__(self, input_channels=1, se_channels=64, reduction=16):
        super().__init__()
        
        # å‰ç½®ç‰¹å¾æå–æ¨¡å—ï¼šå°†è¾“å…¥é€šé“æ‰©å±•ä¸ºå¤šé€šé“
        self.feature_extractor = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šæ‰©å±•é€šé“æ•°
            nn.Conv2d(input_channels, se_channels // 2, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(se_channels // 2),
            nn.ReLU(inplace=True),
            
            # ç¬¬äºŒå±‚ï¼šè¿›ä¸€æ­¥æ‰©å±•é€šé“æ•°
            nn.Conv2d(se_channels // 2, se_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(se_channels),
            nn.ReLU(inplace=True),
        )
        
        # SEæ³¨æ„åŠ›æœºåˆ¶
        # åœ¨ç©ºé—´ç»´åº¦ä¸Š,å°†HÃ—Wå‹ç¼©ä¸º1Ã—1
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # åŒ…å«ä¸¤å±‚å…¨è¿æ¥,å…ˆé™ç»´,åå‡ç»´ã€‚æœ€åæ¥ä¸€ä¸ªsigmoidå‡½æ•°
        self.fc = nn.Sequential(
            nn.Linear(se_channels, se_channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(se_channels // reduction, se_channels, bias=False),
            nn.Sigmoid()
        )
        
        self.se_channels = se_channels

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                init.kaiming_normal_(m.weight, mode='fan_out')
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                init.normal_(m.weight, std=0.001)
                if m.bias is not None:
                    init.constant_(m.bias, 0)

    def forward(self, x):
        # è¾“å…¥: (B, input_channels, H, W) - å¯¹äºMNISTæ˜¯ (B, 1, 28, 28)
        
        # ç‰¹å¾æå–ï¼šå°†è¾“å…¥æ‰©å±•ä¸ºå¤šé€šé“
        features = self.feature_extractor(x)  # (B, se_channels, H, W)
        
        B, C, H, W = features.size()
        
        # SEæ³¨æ„åŠ›æœºåˆ¶
        # Squeeze: (B,C,H,W)-->avg_pool-->(B,C,1,1)-->view-->(B,C)
        y = self.avg_pool(features).view(B, C)
        # Excitation: (B,C)-->fc-->(B,C)-->(B, C, 1, 1)
        y = self.fc(y).view(B, C, 1, 1)
        # scale: (B,C,H,W) * (B, C, 1, 1) == (B,C,H,W)
        out = features * y
        
        return out


class AttentionClassifier(nn.Module):
    """
    å°†æ³¨æ„åŠ›æœºåˆ¶åŒ…è£…æˆåˆ†ç±»å™¨
    é€‚é…MNISTæ•°æ®é›†ï¼šè¾“å…¥ä¸ºå•é€šé“28x28å›¾åƒ
    """
    
    def __init__(self, attention_module, num_classes=10):
        super().__init__()
        self.attention = attention_module
        
        # è·å–æ³¨æ„åŠ›æ¨¡å—è¾“å‡ºçš„é€šé“æ•°
        se_channels = getattr(attention_module, 'se_channels', 64)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(se_channels, num_classes)
        )
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                if m.weight is not None:
                    nn.init.constant_(m.weight, 1)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # è¾“å…¥: (B, 1, 28, 28) for MNIST
        
        # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
        x = self.attention(x)  # (B, se_channels, 28, 28)
        
        # åˆ†ç±»
        x = self.classifier(x)  # (B, num_classes)
        return x


if __name__ == '__main__':
    print("æµ‹è¯•é€‚é…MNISTçš„SEAttentionæ¨¡å—:")
    
    # æµ‹è¯•å•ä¸ªSEAttentionæ¨¡å—
    print("\n1. æµ‹è¯•SEAttentionæ¨¡å—:")
    mnist_input = torch.randn(2, 1, 28, 28)  # MNISTæ ¼å¼è¾“å…¥
    se_attention = SEAttention(input_channels=1, se_channels=64, reduction=16)
    se_output = se_attention(mnist_input)
    print(f"è¾“å…¥å½¢çŠ¶: {mnist_input.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {se_output.shape}")
    
    # æµ‹è¯•ä¸åŒé€šé“æ•°é…ç½®
    print("\n2. æµ‹è¯•ä¸åŒse_channelsé…ç½®:")
    se_attention_32 = SEAttention(input_channels=1, se_channels=32, reduction=8)
    se_output_32 = se_attention_32(mnist_input)
    print(f"se_channels=32, è¾“å‡ºå½¢çŠ¶: {se_output_32.shape}")
    
    # æµ‹è¯•å®Œæ•´åˆ†ç±»å™¨
    print("\n3. æµ‹è¯•å®Œæ•´åˆ†ç±»å™¨:")
    attention = SEAttention(input_channels=1, se_channels=64, reduction=16)
    classifier = AttentionClassifier(attention, num_classes=10)
    
    # æ¨¡æ‹ŸMNISTæ‰¹æ¬¡æ•°æ®
    batch_input = torch.randn(4, 1, 28, 28)
    batch_output = classifier(batch_input)
    print(f"åˆ†ç±»å™¨è¾“å…¥å½¢çŠ¶: {batch_input.shape}")
    print(f"åˆ†ç±»å™¨è¾“å‡ºå½¢çŠ¶: {batch_output.shape}")
    
    # éªŒè¯è¾“å‡ºç»´åº¦æ­£ç¡®æ€§
    assert batch_output.shape == (4, 10), f"æœŸæœ›è¾“å‡ºå½¢çŠ¶ä¸º(4, 10)ï¼Œå®é™…ä¸º{batch_output.shape}"
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼SEAttentionå·²æˆåŠŸé€‚é…MNISTæ•°æ®é›†ã€‚")
    print("ğŸ“Š æ¨¡å‹æ¶æ„ï¼š")
    print(f"   - è¾“å…¥ï¼šå•é€šé“28x28å›¾åƒ")
    print(f"   - ç‰¹å¾æå–ï¼š1 â†’ {se_attention.se_channels}é€šé“")
    print(f"   - SEæ³¨æ„åŠ›ï¼š{se_attention.se_channels}é€šé“")
    print(f"   - åˆ†ç±»è¾“å‡ºï¼š10ç±»")